---
title: "User Guide"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{User Guide}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

## Introduction

At the moment this vignette has the double purpose of keeping track of my
progress and testing the performance of the functions in the package. My aim
is to analyse a large set of high frequency time series measurements of
sunlight at seven different wavebands done with broadband sensors over one
summer.

The data set will be large with seven variables and a few hundred millions of
time points. During daytime measurements were acquired in bursts of 18000 time
points in 15 min every 30 min.

The effect of clouds is intermitent and varies in intensity gradually. Using
quantiles, mean, MAD and variance on the irradiance, and on the running
differences is one possible aproach. However, a recent additional approach
is based on detecting change points in the running differences as a basis to
quantify the duration and amplitude of cloud/shade flecks.

The data are in large text files saved with the software PC400 from Campbell
Scientific, the supplier of the data logger used to collect the data set.

The first step is to read-in the data and split it into "chunks" of data from
18000 consecutive observations at the gaps between the "bursts" of observations,
discarding any incomplete sets and any data measured at a lower frequency. At
this stage running differences for time stamps need to be computed, and running
differences for the measured variables can also be added at the same time.

The smaller values in the running differences are most likely the result of
measurement "noise" and could be "cleaned" before the next steps. The
differences can be used directly or discretized based on their sign.

## Current state

The reading of the data from the 3.7GB text file into a tibble is the slowest
step. Other computations are fast even with the functions are coded fully in R.

## Set up

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(tibble)
library(dplyr)
library(photobiologyInOut)
library(photobiologyFlecks)
library(ggplot2)
```

# Read data

Read data from a CSV file as saved by Campbell Scientific's PC400 program containing data acquired with a CR6 data logger.

```{r}
# file.name <- "data-raw/Viikki Tower_TableMilliSecond.dat"
file.name <- "F:\\aphalo\\research-data\\data-weather\\weather-viikki-field-2025\\data-logged\\data-2025-08-25\\TOA5_1449.TableMilliSecond.dat"
file.size(file.name)
file.mtime(file.name)
```

All rows from a large file will be used in the examples. Reading and decoding the text file is slow, so we create a temporary binary file and use it if it exists or create it after reading the text file if not. Reading from the temp file a 3.7GB tibble takes only about 10 s of both elapsed and user time while reading the DAT file takes nearly 450 s of elapsed time and 3600 s of user time on a computer with a 6 cores processors. The numbers do not add, so user time must be based on logical cores (12) with multithreading. Anyway, 240 s of system time, give 3820 of user + system time, indicating an average use of nearly 9 cores during 7 min 24 s.

```{r}
temp_file.name <- "H:/data-temp/all-chunks-tb.rda"

system.time(
  if (!file.exists(temp_file.name)) {
    # read four chucks of 18000 observations each
    all_chunks.tb <- 
      read_csi_dat(file.name, n_max = Inf) |>
      dplyr::select(-RECORD)
    save(all_chunks.tb, file = temp_file.name)
  } else {
    load(temp_file.name)
  }
)
ncol(all_chunks.tb)
colnames(all_chunks.tb)
nrow(all_chunks.tb)

```

The time series data of sunlight have been measured in 15-min-long bursts separated by 15 min with no data acquisition. Data from each burst will be analysed separately. For this the data frame has to be split at the gaps between bursts.

```{r}
system.time(
  # keep all qty columns, add differences
  chunks.ls <-
    split_chunks(all_chunks.tb,
                 qty.name = NULL,
                 step.len = 0.051,
                 chunk.min.rows = 1.8e4)
)
```

```{r}
str(chunks.ls[[1]], give.attr = FALSE)
```

## Summaries

### Statistics

```{r}
system.time(
  statistics.tb <- chunk_summary(chunks.ls, add.times = TRUE)
)
```

```{r}
statistics.tb |>
  select(time, PAR_Den_CS, parameter) |>
ggplot(aes(time, PAR_Den_CS)) +
  geom_point(size = 0.1) +
  facet_wrap(facets = vars(parameter), scales = "free_y")
```

## Locating change points

### Using `diff()`

In a series without noise, the running difference is equal to zero at the point where the slope changes sign. In a noisy series, even without a consistent change in slope, the differences can locally change sign. In addition, in a series measured at discrete time points, a change in sign must be used rather than a zero value as it may land in-between two time points.

These data are measured under clear sky onditions and, thus, no cloud flecks are detected using the running differences method.

Once denoised, changes in slope can be detected using `sign()` to discretize the data into values indicating decrease (-1) , no-change (0) and increase (+1).

```{r}
chunks_dn.ls <- 
  denoise_chunk(chunks.ls, ignore.range = 0.1, add.signs = TRUE)
```


### Change points

Subsequently, changes in slope can be located using `rle()`. In this example all differences are smaller than 5 per thousand of the mean irradiance.

```{r}
sign_changes.ls <- list()
for (i in names(chunks_dn.ls)) {
  sign_changes.ls[[i]] <- 
    list(rle = rle(chunks_dn.ls[[i]][["PAR_Den_CS.sign"]]))
}
length(sign_changes.ls)
```

